# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ILDTLCtI-xve_0gVd1_OV-CkZpgxIq9U
"""

# Install the necessary libraries
!pip install transformers
!pip install torch
!pip install pandas
!pip install scikit-learn

# Import the required libraries
import pandas as pd
import torch
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# Load the dataset
file_path = '/content/corpsolutions_data.csv'
df = pd.read_csv(file_path)

# Drop rows with missing values
df.dropna(inplace=True)

# Initialize the label encoder
label_encoder = LabelEncoder()
df['Label'] = label_encoder.fit_transform(df['Answers '])

# Save the label encoder
pd.to_pickle(label_encoder, '/content/label_encoder.pkl')

# Tokenization and encoding function
def tokenize_and_encode(tokenizer, text, max_length=128):
    encoding = tokenizer.encode_plus(
        text,
        max_length=max_length,
        truncation=True,
        padding='max_length',
        return_tensors='pt',
        return_attention_mask=True,
    )
    return encoding

# Initialize the BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenize and encode the questions
input_ids = []
attention_masks = []
labels = []

for idx, row in df.iterrows():
    question = row['Questions ']
    label = row['Label']
    encoding = tokenize_and_encode(tokenizer, question)

    input_ids.append(encoding['input_ids'])
    attention_masks.append(encoding['attention_mask'])
    labels.append(label)

input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
labels = torch.tensor(labels)

# Split the data into training and validation sets
train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(
    input_ids, attention_masks, labels, test_size=0.1)

train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=8)

val_data = TensorDataset(val_inputs, val_masks, val_labels)
val_sampler = SequentialSampler(val_data)
val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=8)

# Load pre-trained BERT model for sequence classification
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))

# Set up the optimizer
optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)

# Move model to GPU if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Train the model
epochs = 19
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for step, batch in enumerate(train_dataloader):
        batch_input_ids, batch_attention_masks, batch_labels = tuple(t.to(device) for t in batch)

        outputs = model(input_ids=batch_input_ids,
                        attention_mask=batch_attention_masks,
                        labels=batch_labels)

        loss = outputs.loss
        total_loss += loss.item()

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    avg_train_loss = total_loss / len(train_dataloader)
    print(f'Epoch {epoch+1}, Loss: {avg_train_loss}')

    # Validation
    model.eval()
    eval_loss = 0
    for batch in val_dataloader:
        batch_input_ids, batch_attention_masks, batch_labels = tuple(t.to(device) for t in batch)

        with torch.no_grad():
            outputs = model(input_ids=batch_input_ids,
                            attention_mask=batch_attention_masks,
                            labels=batch_labels)

        loss = outputs.loss
        eval_loss += loss.item()

    avg_val_loss = eval_loss / len(val_dataloader)
    print(f'Validation Loss: {avg_val_loss}')

# Save the fine-tuned model
model.save_pretrained('/content/fine_tuned_bert')
tokenizer.save_pretrained('/content/fine_tuned_bert')

#chatbot

# Install the necessary libraries
!pip install transformers
!pip install torch
# Install the necessary libraries
!pip install transformers
!pip install torch
!pip install pandas

# Import the required libraries
import torch
import pandas as pd
from transformers import BertTokenizer, BertForSequenceClassification

# Load the fine-tuned model and tokenizer
model_dir = '/content/fine_tuned_bert'
tokenizer = BertTokenizer.from_pretrained(model_dir)
model = BertForSequenceClassification.from_pretrained(model_dir)

# Load label encoder to decode predictions
label_encoder = pd.read_pickle('/content/label_encoder.pkl')

def classify_input(user_input):
    # Tokenize the user input
    inputs = tokenizer(user_input, return_tensors='pt', truncation=True, padding=True, max_length=128)

    # Perform inference
    model.eval()
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        predicted_class_id = torch.argmax(logits, dim=1).item()

    # Decode the predicted class
    predicted_class = label_encoder.inverse_transform([predicted_class_id])[0]
    return predicted_class

def main():
    print("Chatbot is ready! Type 'exit' to end the chat.")
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'exit':
            print("Chatbot: Goodbye!")
            break
        response = classify_input(user_input)
        print(f"Chatbot: {response}")

if __name__ == '__main__':
    main()